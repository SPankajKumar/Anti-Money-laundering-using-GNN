{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5080714,"sourceType":"datasetVersion","datasetId":2948142}],"dockerImageVersionId":30673,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom pandas import DataFrame\nfrom pandas import concat\nimport numpy as np\nfrom numpy import concatenate\nfrom math import sqrt\nfrom sklearn.preprocessing import LabelEncoder\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns","metadata":{"id":"FqkFzVvF7qJk","execution":{"iopub.status.busy":"2024-06-03T04:33:57.355606Z","iopub.execute_input":"2024-06-03T04:33:57.356202Z","iopub.status.idle":"2024-06-03T04:34:00.764982Z","shell.execute_reply.started":"2024-06-03T04:33:57.356168Z","shell.execute_reply":"2024-06-03T04:34:00.763856Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"folder_path='/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/HI-Small_Trans.csv'\ndf = pd.read_csv(folder_path)\nprint(len(df.index))\n","metadata":{"id":"3oXiX-xS8LRu","outputId":"7a5cf513-0418-4837-f368-f541c9fb7f20","execution":{"iopub.status.busy":"2024-06-03T04:34:00.766850Z","iopub.execute_input":"2024-06-03T04:34:00.767372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()\n","metadata":{"id":"h32r7nUF_pvO","outputId":"f9d66a4c-c8c9-4609-f235-cfe27d8675c3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.dtypes)\n","metadata":{"id":"D3jBZv0B_tG1","outputId":"80f22fd2-6e81-46a7-9678-4e0b5d479b5d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Timestamp'] = pd.to_datetime(df['Timestamp'])\ndf['Amount Received'] = df['Amount Received'].astype('float32')\ndf['Amount Paid'] = df['Amount Paid'].astype('float32')\ndf['Is Laundering'] = df['Is Laundering'].astype('float32')\ndf.dtypes","metadata":{"id":"aIgTIEV-Jrx-","outputId":"8cc54e84-3c50-4ef7-b6a0-d56ae3b8eb1b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.isnull().sum())\n","metadata":{"id":"EveqQWvo_sMO","outputId":"ebc38b9f-1add-406f-c6ce-d384514fb592","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"not_equal1 = df.loc[~(df['Amount Received'] == df['Amount Paid'])]\nnot_equal2 = df.loc[~(df['Receiving Currency'] == df['Payment Currency'])]\nprint(not_equal1)\nprint(not_equal2)","metadata":{"id":"WDQc4QjwAAxN","outputId":"e9a8e673-1319-47ae-96e1-712f791ab7ac","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sorted(df['Receiving Currency'].unique()))\nprint(sorted(df['Payment Currency'].unique()))","metadata":{"id":"Z1mCRTO_ATyc","outputId":"abf0ce94-02f2-4950-999a-4a8ed189f134","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in df.columns:\n    print(col,'\\n')\n    print(df[col].nunique(), '\\n')\n    print(df[col].value_counts())","metadata":{"id":"iexZkJV8AV89","outputId":"ff80f123-cbf2-4d64-df09-d793bf2c0937","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_col  = {'Account':'Sender','Account.1':'Receiver','Receiving Currency':'Receiving_Currency',\n            'Payment Currency':'Payment_Currency','Payment Format':'Payment_Format','From Bank':'From_Bank',\n            'To Bank':'To_Bank','Amount Received':'Amount_Received','Amount Paid':'Amount_Paid','Is Laundering':'Fraud'}\ndf.rename(columns=new_col, inplace=True)\ndf.head(3)","metadata":{"id":"nrS988wIHXH3","outputId":"259acc46-5c63-400f-f1dd-d2b4103f9c67","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\n\n#label encoding\ndef df_label_encoder(df, columns):\n        le = preprocessing.LabelEncoder()\n        for i in columns:\n            df[i] = le.fit_transform(df[i].astype(str))\n        return df","metadata":{"id":"FLQRBDXrCPgk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(df):\n        df = df_label_encoder(df,['Payment_Format', 'Payment_Currency', 'Receiving_Currency','Fraud'])\n        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n\n\n\n        receiving_df = df[['Receiver', 'Amount_Received', 'Receiving_Currency','To_Bank']]\n        paying_df = df[['Sender', 'Amount_Paid', 'Payment_Currency','From_Bank']]\n        currency_ls = sorted(df['Receiving_Currency'].unique())\n\n        return df, receiving_df, paying_df, currency_ls","metadata":{"id":"Kn4ot9imCsx1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['date']=pd.to_datetime(df['Timestamp']).dt.day","metadata":{"id":"SIvayk01EuNO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['time']=(df['Timestamp']).dt.hour\ndf.drop(['Timestamp'],axis=1)","metadata":{"id":"qpB7quIoFCP2","outputId":"8ddad42e-3b08-41db-c86a-fbb2b65c57b3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df, receiving_df, paying_df, currency_ls = preprocess(df=df )\nprint(df.head())","metadata":{"id":"mwCoSkzLFuZe","outputId":"6aac75e6-9c07-40e6-b3c3-56164f099864","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(receiving_df.head())\nprint(paying_df.head())\nprint(currency_ls)","metadata":{"id":"PHAom_h3JlL_","outputId":"b33c5e6f-b389-4028-f11a-e143ad40a71a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"id":"iTcnVVAzkpJ-","outputId":"6a7f410e-f33b-42b6-bf64-c18ce939b69d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Input, Dense\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.layers import Dropout\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Activation\nfrom keras.layers import Concatenate\nfrom keras.layers import AveragePooling1D\nfrom keras.layers import GlobalMaxPooling1D\nfrom keras.layers import GlobalAveragePooling1D\nfrom keras.layers import Conv1D\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport torch.nn as nn\n","metadata":{"id":"lgfiHrkK-djW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import networkx as nx","metadata":{"id":"-uBYYXULHBXr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Add edges and properties to the edges \nfor _, row in df.iterrows():\n    # Create a variable for each properties for each edge\n\n        Fraud = row[\"Fraud\"],\n        Payment_Format = row[\"Payment_Format\"],\n        Payment_Currency = row[\"Payment_Currency\"],\n        Amount_Paid  = row[\"Amount_Paid\"],\n        Receiving_Currency =row[\"Receiving_Currency\"],\n        Amount_Received    = row[\"Amount_Received\"],\n        To_Bank =  row[\"To_Bank\"],\n        From_Bank = row[\"From_Bank\"],\n        date =  row[\"date\"],\n        time = row['time']\n    \n \n        G.add_edge(row['Sender'], row['Receiver'], Fraud = Fraud , Payment_Format = Payment_Format , Payment_Currency = Payment_Currency ,\n              Amount_Paid = Amount_Paid , Receiving_Currency = Receiving_Currency , Amount_Received = Amount_Received , To_Bank = To_Bank ,\n              From_Bank = From_Bank , date = date , time = time)\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an empty graph\nG = nx.MultiGraph()\n\nfor index, row in df.iterrows():\n    G.add_edge(row['Sender'], row['Receiver'], \n               Amount_Paid=row['Amount_Paid'], Amount_Received=row['Amount_Received'], \n               Receiving_Currency=row['Receiving_Currency'], Payment_Currency=row['Payment_Currency'],\n               From_Bank=row['From_Bank'], To_Bank=row['To_Bank'],Payment_Format = row[\"Payment_Format\"])\n\n# Node features\nnode_features = {\n    'avg_transaction_amount': df.groupby('Sender')['Amount_Paid'].mean().fillna(0).to_dict(),\n    'total_amount_received': df.groupby('Receiver')['Amount_Received'].sum().fillna(0).to_dict(),\n    'total_amount_paid': df.groupby('Sender')['Amount_Paid'].sum().fillna(0).to_dict(),\n    'frequency_of_transactions': df['Sender'].value_counts().to_dict(),\n    'unique_receiving_currencies': df.groupby('Receiver')['Receiving_Currency'].nunique().to_dict(),\n    'unique_paying_receiving': df.groupby('Sender')['Payment_Currency'].nunique().to_dict(),\n    'unique_banks_used': df.groupby('Sender')[['From_Bank', 'To_Bank']].nunique().sum(axis=1).to_dict()\n}\n\n# Convert node features to a matrix\nnode_features_matrix = np.array([\n    [node_features['avg_transaction_amount'].get(node, 0),\n     node_features['total_amount_received'].get(node, 0),\n     node_features['total_amount_paid'].get(node, 0),\n     node_features['frequency_of_transactions'].get(node, 0),\n     node_features['unique_receiving_currencies'].get(node, 0),\n     node_features['unique_paying_receiving'].get(node, 0),\n     node_features['unique_banks_used'].get(node, 0)]\n    for node in G.nodes()\n])\n\nprint(\"Node Features:\")\nprint(node_features_matrix)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Edge features\nedge_features = {\n    'total_amount_transferred': df.groupby(['Sender', 'Receiver']).agg(\n        {'Amount_Paid': 'sum', 'Amount_Received': 'sum'}\n    ).sum(axis=1).fillna(0).to_dict(),\n    'common_receiving_currency': df.groupby(['Sender', 'Receiver'])['Receiving_Currency'].nunique().to_dict(),\n    'common_payment_currency': df.groupby(['Sender', 'Receiver'])['Payment_Currency'].nunique().to_dict(),\n    'common_banks_used': df.groupby(['Sender', 'Receiver'])[['From_Bank', 'To_Bank']].nunique().sum(axis=1).to_dict(),\n    'common_payment_format': df.groupby(['Sender', 'Receiver'])['Payment_Format'].nunique().to_dict()\n}\n\n# Convert edge features to a matrix\nedge_features_matrix = np.array([\n    [edge_features['total_amount_transferred'].get((u, v), 0),\n     edge_features['common_receiving_currency'].get((u, v), 0),\n     edge_features['common_payment_currency'].get((u, v), 0),\n     edge_features['common_banks_used'].get((u, v), 0),\n     edge_features['common_payment_format'].get((u, v), 0)]\n    \n    for u, v in G.edges()\n])\n\nprint(\"Edge Features:\")\nprint(edge_features_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert node features to tensor\nnode_features_tensor = torch.tensor(node_features_matrix, dtype=torch.float32)\n\n# Labels\nlabels = torch.tensor(df['Fraud'].values, dtype=torch.float32).view(-1, 1)\n\n# Align labels with node features\nnum_nodes = node_features_tensor.shape[0]\nsliced_labels = labels[:num_nodes, :]\n\n# Check if shapes match\nif node_features_tensor.shape[0] == sliced_labels.shape[0]:\n    print(\"Shapes Match!\")\nelse:\n    print(\"Shapes Still Mismatched :(\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nsample_size = 5\nfor i, edge in enumerate(G.edges()):\n    print(G.get_edge_data(*edge))\n    if i >= sample_size - 1:\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare the data for input into the model\nedge_list = list(G.edges(data=True))\nlist(edge_list[i][2].values())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Define a simple GNN model\nclass GNN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(GNN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return x\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nfrom sklearn.model_selection import train_test_split\n# Initialize model, loss function, and optimizer\ninput_dim = node_features_matrix.shape[1]\nhidden_dim = 64\noutput_dim = 1  # binary classification\nmodel = GNN(input_dim, hidden_dim, output_dim)\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n\nprint(\"Node Features Shape:\", node_features_tensor.shape)\nprint(\"Labels Shape:\", sliced_labels.shape)\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(node_features_tensor, sliced_labels, test_size=0.2, random_state=42)\n\n# Standardize node features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Training loop\nepochs = 100\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    outputs = model(torch.tensor(X_train, dtype=torch.float32))\n    loss = criterion(outputs, y_train)\n    loss.backward()\n    optimizer.step()\n    \n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n\n# Evaluate the model on test set\nwith torch.no_grad():\n    outputs = model(torch.tensor(X_test, dtype=torch.float32))\n    predicted = (outputs > 0.5).float()\n    accuracy = (predicted == y_test).float().mean()\n    print(f\"Accuracy on test set: {accuracy.item()*100:.2f}%\")\n\n# Evaluate the original model on the test set\n    outputs_original = model(torch.tensor(X_test, dtype=torch.float32))\n    predicted_original = (outputs_original > 0.5).float()\n    \n    # Convert tensors to numpy arrays\n    y_test_np = y_test.numpy().flatten()\n    predicted_original_np = predicted_original.numpy().flatten()\n    \n    # Compute accuracy\n    accuracy = accuracy_score(y_test_np, predicted_original_np)\n    print(f\"Accuracy on test set: {accuracy*100:.2f}%\")\n    \n    # Compute confusion matrix\n    cm = confusion_matrix(y_test_np, predicted_original_np)\n    print(\"Confusion Matrix:\")\n    print(cm)\n    \n    # Compute classification report\n    print(\"Classification Report:\")\n    print(classification_report(y_test_np, predicted_original_np))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Define the FraudGNN model\nclass FraudGNN(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(FraudGNN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.dropout = nn.Dropout(0.5)  # Dropout layer for regularization\n        self.fc2 = nn.Linear(hidden_dim, 1)\n        \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = torch.sigmoid(self.fc2(x))\n        return x.squeeze(-1)\n\n# Oversample using SMOTE\nsmote = SMOTE()\nnode_features_resampled, sliced_labels_resampled = smote.fit_resample(node_features_tensor.numpy(), sliced_labels.numpy())\n\n# Standardize features\nscaler = StandardScaler()\nnode_features_resampled = scaler.fit_transform(node_features_resampled)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(node_features_resampled, sliced_labels_resampled, test_size=0.2, random_state=42)\n\n# Initialize model\ninput_dim = node_features_resampled.shape[1]\nhidden_dim = 64\nmodel = FraudGNN(input_dim, hidden_dim)\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training\nepochs = 10\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    outputs = model(torch.tensor(X_train, dtype=torch.float32))\n    loss = criterion(outputs, torch.tensor(y_train, dtype=torch.float32))\n    loss.backward()\n    optimizer.step()\n\n# Evaluate\nwith torch.no_grad():\n    outputs = model(torch.tensor(X_test, dtype=torch.float32))\n    predicted = (outputs > 0.5).float()\n    accuracy = (predicted == torch.tensor(y_test, dtype=torch.float32)).float().mean()\n    print(f\"Accuracy on test set: {accuracy.item()*100:.2f}%\")\n\n# Confusion matrix and classification report\nprint(confusion_matrix(y_test, predicted))\nprint(classification_report(y_test, predicted))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Split data into train, validation, and test sets\nX_train, X_temp, y_train, y_temp = train_test_split(node_features_tensor, sliced_labels, test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\ninput_dim = node_features_tensor.shape[1]  \nhidden_dim = 64\nmodel = FraudGNN(input_dim, hidden_dim)\ncriterion = nn.BCEWithLogitsLoss()  # Use logits for stability\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop with validation\ntrain_losses = []\nval_losses = []\n\nepochs = 100\nfor epoch in range(epochs):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(X_train)\n    \n    # Squeeze the target labels to match the output shape\n    loss = criterion(outputs, y_train.squeeze(-1))\n    loss.backward()\n    optimizer.step()\n    train_losses.append(loss.item())\n    \n    # Validation\n    model.eval()\n    with torch.no_grad():\n        outputs_val = model(X_val)\n        \n        # Squeeze the target labels for validation\n        val_loss = criterion(outputs_val, y_val.squeeze(-1))\n        val_losses.append(val_loss.item())\n    \n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n\n    \n    \n# Plotting the losses\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Val Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss Over Time')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    outputs = model(torch.tensor(X_test, dtype=torch.float32))\n    predicted = (outputs > 0.5).float()\n    accuracy = (predicted == torch.tensor(y_test, dtype=torch.float32)).float().mean()\n    print(f\"Accuracy on test set: {accuracy.item()*100:.2f}%\")\n\n# Confusion matrix and classification report\nprint(confusion_matrix(y_test, predicted))\nprint(classification_report(y_test, predicted))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import networkx as nx\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create a graph\nG = nx.Graph()\n\n# Add nodes with their features\nfor i, features in enumerate(node_features_resampled):\n    G.add_node(i, features=features)\n\n# Predict labels for nodes\nwith torch.no_grad():\n    outputs = model(torch.tensor(node_features_resampled, dtype=torch.float32))\n    predicted_labels = (outputs > 0.5).float().numpy()\n\n# Draw the graph\npos = nx.spring_layout(G, seed=42)  # Position nodes using Fruchterman-Reingold force-directed algorithm\n\n# Color nodes based on predicted labels\nnode_colors = ['blue' if label == 0 else 'red' for label in predicted_labels]\n\nnx.draw(G, pos, with_labels=False, node_size=50, width=0.1, node_color=node_colors)\n\nplt.show()\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# Define the Enhanced FraudGNN model\nclass EnhancedFraudGNN(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(EnhancedFraudGNN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(128, 64)\n        self.dropout2 = nn.Dropout(0.3)\n        self.fc3 = nn.Linear(64, 32)\n        self.fc4 = nn.Linear(32, 1)\n        \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.relu(self.fc3(x))\n        x = torch.sigmoid(self.fc4(x))\n        return x.squeeze(-1)\n\n# Oversample using SMOTE\nsmote = SMOTE()\nnode_features_resampled, sliced_labels_resampled = smote.fit_resample(node_features_tensor.numpy(), sliced_labels.numpy())\n\n# Standardize features\nscaler = StandardScaler()\nnode_features_resampled = scaler.fit_transform(node_features_resampled)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(node_features_resampled, sliced_labels_resampled, test_size=0.2, random_state=42)\n\n# Initialize model\ninput_dim = node_features_resampled.shape[1]\nmodel = EnhancedFraudGNN(input_dim, 64)\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.1)\n\n# Training with early stopping\nbest_val_loss = float('inf')\npatience = 0\nfor epoch in range(100):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(torch.tensor(X_train, dtype=torch.float32))\n    loss = criterion(outputs, torch.tensor(y_train, dtype=torch.float32))\n    loss.backward()\n    optimizer.step()\n    \n    model.eval()\n    with torch.no_grad():\n        outputs_val = model(torch.tensor(X_test, dtype=torch.float32))\n        val_loss = criterion(outputs_val, torch.tensor(y_test, dtype=torch.float32))\n    \n    scheduler.step(val_loss)\n    \n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        patience = 0\n    else:\n        patience += 1\n        if patience > 10:\n            print(\"Early stopping...\")\n            break\n\n# Evaluate\nwith torch.no_grad():\n    outputs = model(torch.tensor(X_test, dtype=torch.float32))\n    predicted = (outputs > 0.5).float()\n    accuracy = (predicted == torch.tensor(y_test, dtype=torch.float32)).float().mean()\n    print(f\"Accuracy on test set: {accuracy.item()*100:.2f}%\")\n\n# Confusion matrix and classification report\nprint(confusion_matrix(y_test, predicted))\nprint(classification_report(y_test, predicted))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Split data into train, validation, and test sets\nX_train, X_temp, y_train, y_temp = train_test_split(node_features_tensor, sliced_labels, test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\ninput_dim = node_features_tensor.shape[1]  \nhidden_dim = 64\nmodel = EnhancedFraudGNN(input_dim, hidden_dim)\ncriterion = nn.BCEWithLogitsLoss()  # Use logits for stability\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop with validation\ntrain_losses = []\nval_losses = []\n\nepochs = 100\nfor epoch in range(epochs):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(X_train)\n    \n    # Squeeze the target labels to match the output shape\n    loss = criterion(outputs, y_train.squeeze(-1))\n    loss.backward()\n    optimizer.step()\n    train_losses.append(loss.item())\n    \n    # Validation\n    model.eval()\n    with torch.no_grad():\n        outputs_val = model(X_val)\n        \n        # Squeeze the target labels for validation\n        val_loss = criterion(outputs_val, y_val.squeeze(-1))\n        val_losses.append(val_loss.item())\n    \n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n\n    \n    \n# Plotting the losses\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Val Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss Over Time')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}